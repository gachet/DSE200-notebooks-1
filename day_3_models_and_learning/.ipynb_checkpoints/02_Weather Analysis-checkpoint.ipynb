{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Analysis of a NOAA weather dataset ###\n",
    "\n",
    "In this notebook we are analyzing a sample out of data that was downloaded from http://www1.ncdc.noaa.gov/pub/data/ghcn/daily/, the main file is ghcnd_all.tar.gz which is about 2.4 GB which becomes around 20GB when uncompressed.\n",
    "\n",
    "The data contains about 1 million station-year recordings. That is too much to analyzer on single core machine, so we start by taking a sample of 20,000 recordings of the maximal daily temperatures for a period of a 365 days starting on January 1st (the last day of leap years is discarded)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Checking the versions of some important packages ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as sk\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print 'pandas version: ',pd.__version__\n",
    "print 'numpy version:',np.__version__\n",
    "print 'sklearn version:',sk.__version__\n",
    "print 'matplotlib version:', matplotlib.__version__"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Versions are expected to be:\n",
    "pandas version:  0.20.3\n",
    "numpy version: 1.13.3\n",
    "sklearn version: 0.18.2\n",
    "matplotlib version: 2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if a version of a module is too old, you can use the following command to update it\n",
    "#!pip install --upgrade matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data loading and analysis\n",
    "Switch to the data directory and check it's contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "%cd ../data/weather\n",
    "!ls -lh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!cat data-source.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### The main files\n",
    "- *data-source.txt* - information about downloading the data from NOAA\n",
    "- *ghcnd-readme.txt* - A readme file describing the content of all of the files from ghcnd, in particular:\n",
    "- *ghcnd-stations.txt* - information about each of the meteorological stations.\n",
    "- *Sample_TMAX* - a file with 10,000 randomly selected one-year-long TMAX measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# read a single line in the data file\n",
    "file=open('SAMPLE_TMAX.csv','r')\n",
    "for line in file.readlines():\n",
    "    print line\n",
    "    print len(line.split(','))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### read data into a Pandas Dataframe ##\n",
    "* Read the data into a DataFrame\n",
    "* Read the data vectors in G\n",
    "* Divide by 10.0 to get the temperatude in degrees celsius\n",
    "* Replace values outside the range [-400,500]  ([-40,50] degrees celsius) with nan  \n",
    "* Paste fixed matrix back into Dout\n",
    "* Show the first few lines of DDout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create an index that is a range of dates\n",
    "#using 2001 which was not a leap year (2000 was)\n",
    "days_index=pd.date_range('January 1, 2001', periods=365,freq='D')\n",
    "days=list(days_index)\n",
    "days[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=['station','measurement','year']+days\n",
    "Data = pd.read_csv('SAMPLE_TMAX.csv',header=None,names=columns)\n",
    "Data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting rows and columns in a dataframe\n",
    "\n",
    "* To select a set of columns from a dataframe:\n",
    "```\n",
    "DF[['column2','column6','column1']]\n",
    "```\n",
    "This returns a new dataframe containing the selected columns in the selected order.\n",
    "\n",
    "* To select a set of rows based on a condition defined using one of the coluns:\n",
    "```\n",
    "DF[DF['column1']>5]\n",
    "```\n",
    "\n",
    "More sophisticated selections can be down using:\n",
    "* `.loc` : select according to the **name** of the column or row.\n",
    "* `.iloc` : select according to the **position** of the column or row.\n",
    "\n",
    "Resources:\n",
    "* [A tutorial on selecting rows and columns in a dataframe using iloc, loc and](https://pandas.pydata.org/pandas-docs/stable/indexing.html)\n",
    "* [Pandas Cheat Sheet](https://github.com/pandas-dev/pandas/raw/master/doc/cheatsheet/Pandas_Cheat_Sheet.pdf) (newer and better than the one in the folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# some data cleaning\n",
    "G=Data.loc[:,days]\n",
    "G[G<-400]=np.nan\n",
    "G[G>500]=np.nan\n",
    "G=G/10\n",
    "Data.loc[:,days]=G\n",
    "G=G.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# compute the histogram\n",
    "tmp=G.loc[:,:].unstack()\n",
    "print 'shape(tmp)=',shape(tmp),'type(tmp)=',type(tmp)\n",
    "tmp.hist(bins=50);\n",
    "title('overall distribution of temperatures')\n",
    "print 'min=%3.2f,max=%3.2f'%(tmp.min(),tmp.max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Plotting temperature as a function of day-of-year\n",
    "\n",
    "### Script for plotting yearly plots ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def YearlyPlots(T,ttl='',size=(15,10)):\n",
    "    if shape(T)[0] != 365:\n",
    "        raise ValueError(\"First dimension of T should be 365. Shape(T)=\"+str(shape(T)))\n",
    "    T.set_index(days_index)\n",
    "    T.plot(legend=False,figsize=size)\n",
    "    # rotate and align the tick labels so they look better\n",
    "    gcf().autofmt_xdate()\n",
    "    ylabel('temperature')\n",
    "    grid()\n",
    "    title(ttl)\n",
    "YearlyPlots(Data.loc[20:30, days].transpose(),ttl='A sample of yearly plots')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Plots for sydney, Australia ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "sydneyStations=['ASN00066' in station for station in Data['station']]\n",
    "print Data[sydneyStations]['station'].values\n",
    "tmp=Data[sydneyStations].transpose()\n",
    "print shape(tmp)\n",
    "YearlyPlots(tmp.loc[days,:],ttl='Sydney Stations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computing statistics when there are missing values\n",
    "\n",
    "### Computing mean and std for each station/year ###\n",
    "And calculating the standard deviation for each year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# a simple scale function to normalize the data-frame row-by-row\n",
    "from numpy import mean, std\n",
    "def Compute_mean_std(Din):\n",
    "    matrix=Din.loc[:,days]\n",
    "    Din['Mean']=mean(matrix, axis=1).values \n",
    "    Din['Std']=std(matrix, axis=1).values\n",
    "    return Din\n",
    "\n",
    "if 'measurement' in Data.columns:\n",
    "    Data=Data.drop('measurement',axis=1)  # remove column that is the constant TMAX\n",
    "Dout=Compute_mean_std(Data)\n",
    "#reorder the columns\n",
    "Dout=Dout[['station','year','Mean','Std']+days]\n",
    "Dout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Compute average temperature for each day of the year. ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Mean=pd.DataFrame(mean(Dout.loc[:,days], axis=0)) \n",
    "YearlyPlots(Mean,ttl='The global mean temperature for each day of the year')\n",
    "shape(Mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Distribution of missing values\n",
    "We find the distribution of missing values and decide how to deal with them. From the analysis below we see that most rows have some\n",
    "missing values. We therefor choose to perform the average more carefully, rather than discard rows with many missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "nan_per_row=sum(isnan(Dout.iloc[:,1:365]),axis=1) \n",
    "nan_per_row.hist(bins=100)\n",
    "sum(nan_per_row>50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Computing the covariance matrix in a NaN-tolerant fashion\n",
    "We compute the empirical covariance matrix in a way that tolerates small numbers of missing values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(nan_per_row>50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "max_nan=50\n",
    "\n",
    "M=Dout.loc[:,days].transpose()\n",
    "#M.drop(nan_per_row>0,axis=1,inplace=True)\n",
    "M=M.loc[:,nan_per_row<max_nan]\n",
    "Dout=Dout.loc[nan_per_row<max_nan,:]\n",
    "Dout.index=range(shape(Dout)[0])\n",
    "(columns,rows)=shape(M)\n",
    "Mean=mean(M, axis=1).values\n",
    "\n",
    "print (columns,rows), shape(Mean),shape(M),shape(Dout)\n",
    "C=np.zeros([columns,columns])   # Sum\n",
    "N=np.zeros([columns,columns])   # Counter of non-nan entries\n",
    "Dout.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#%%time\n",
    "for i in range(rows):\n",
    "    if i % 1000==0: \n",
    "        print i\n",
    "    row=M.iloc[:,i]-Mean;\n",
    "    outer=np.outer(row,row)\n",
    "    valid=isnan(outer)==False\n",
    "    C[valid]=C[valid]+outer[valid]  # update C with the valid location in outer\n",
    "    N[valid]=N[valid]+1\n",
    "#valid_outer=np.multiply(1-isnan(N),N>0)\n",
    "cov=np.divide(C,N)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Singular Value Decomposition (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "shape(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "U,D,V=np.linalg.svd(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "shape(U),shape(D),shape(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Percentage of variance Explained ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "plot(cumsum(D[:])/sum(D))\n",
    "xlim([0,365])\n",
    "grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k=5 # number of components to show.\n",
    "DF_U=pd.DataFrame(U[:,:k])\n",
    "print shape(DF_U)\n",
    "YearlyPlots(DF_U,ttl='The most significant eigen-vectors')\n",
    "legend(range(0,k));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "k=5\n",
    "Eig=np.matrix(U[:,:k])\n",
    "print 'checking that the norm of the eigenvectors is always 1'\n",
    "print ['%6.3f,'%np.linalg.norm(U[:,i]) for i in range(k)]\n",
    "matrix=np.matrix(Dout.loc[:,days])-Mean.transpose()\n",
    "\n",
    "#replacing nans with zeros\n",
    "matrix[isnan(matrix)]=0\n",
    "print shape(Eig),shape(matrix)\n",
    "Prod=matrix*Eig;\n",
    "print shape(Prod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Insert coefficients for k top eigenvectors into the dataframe **Dout**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "for i in range(k-1,-1,-1):\n",
    "    Ser=pd.Series(np.array(Prod)[:,i],index=Dout.index)\n",
    "    Dout.insert(4,'V'+str(i),Ser)\n",
    "Dout.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Geographic location of stations\n",
    "Loading the station longitude/latitude and merging it into the Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!cat ghcnd-readme.txt   # uncomment to read the readme file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "------------------------------\n",
    "Variable   Columns   Type\n",
    "------------------------------\n",
    "ID            1-11   Character\n",
    "LATITUDE     13-20   Real\n",
    "LONGITUDE    22-30   Real\n",
    "ELEVATION    32-37   Real\n",
    "STATE        39-40   Character\n",
    "NAME         42-71   Character\n",
    "GSNFLAG      73-75   Character\n",
    "HCNFLAG      77-79   Character\n",
    "WMOID        81-85   Character\n",
    "------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Make all lines be of length 90 to solve problem wilth read_fwf\n",
    "out=open('ghcnd-stations_buffered.txt','w')\n",
    "for line in open('ghcnd-stations.txt','r').readlines():\n",
    "    line=line.rstrip()\n",
    "    string=line+' '*(90-len(line))+'\\n'\n",
    "    out.write(string)\n",
    "out.close()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A grid to define the character locations of the fields\n",
    "\n",
    "AE000041196  25.3330   55.5170   34.0    SHARJAH INTER. AIRP            GSN     41196\n",
    "0123456789012345678901234567890123456789012345678901234567890123456789012345678901234567890\n",
    "0         1         2         3         4         5         6         7         8         9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "colspecs = [(0, 11), (11, 21), (21, 31), (31, 38),(39,41),(41,72),(72,76),(76,80),(80,86)]\n",
    "stations = pd.read_fwf('ghcnd-stations_buffered.txt', colspecs=colspecs, header=None, index_col=0,\n",
    "                       names=['latitude','longitude','elevation','state','name','GSNFLAG','HCNFLAG','WMOID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#stations['elevation'][stations['elevation']==-999.9]=0  # decided not to remove -999.9 because this confuses hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "stations.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### perform a **JOIN** ###\n",
    "Join the geographical information into **Dout**, creating a new dataframe called **Djoined**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Djoined=Dout.join(stations,on='station')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "Djoined.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Djoined['AbsLatitude']=abs(Djoined['latitude'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Djoined.loc[:5,['station',u'longitude','latitude',u'elevation',u'AbsLatitude','Mean','Std','V0','V1','V2']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Looking for significant correlations and dependencies\n",
    "Each station is no described by a small number of features. We would like to understand the dependencies between these features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Reduced=Djoined[['latitude','elevation','Mean','Std','V0','V1','V2','V3','V4']]\n",
    "Reduced.cov()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<span style=\"color:red\"> The correlations between different $V_i$ components should be zero, which it isn't.\n",
    "Is this due to numerical roundoff errors? Are the correlations statistically significant for this sample size? </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Reduced.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Choosing significance threshold so that none of the correlations between the Vi-s are significant.\n",
    "abs(Reduced.corr())>0.2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(Reduced, alpha=0.03, figsize=(20, 20), diagonal='kde');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X='latitude'\n",
    "Djoined.loc[:,X].hist(bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X='Mean';Y='V0'\n",
    "figure(figsize=(10,10))\n",
    "scatter(Djoined.loc[:,X],Djoined.loc[:,Y],alpha=0.05)\n",
    "xlabel(X)\n",
    "ylabel(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#checking for an anomaly in the elevations of stations\n",
    "Djoined[['station','elevation']][Djoined['elevation']<-500].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "!grep ASN00010865 ghcnd-stations.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a pickle file for the maps notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "Dict={\n",
    "    'Djoined':Djoined,\n",
    "    'stations':stations\n",
    "     }\n",
    "pickle.dump(Dict,open('weather.pkl','wb'),protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Data Reconstruction\n",
    "\n",
    "Using PCA, and based on the fact that the top eigenvectors explain most of the variance, we have represented each (year,station) temperature sequence (365 numbers) using just 5 numbers. We can use the associated eigen-vectors to construct and approximation of the original temperature sequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pd.DataFrame(np.ones(Kprod.shape[0],1)*KMean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "KMean=np.array([Mean])\n",
    "Recon0=pd.DataFrame(np.ones([Kprod.shape[0],1])*KMean)\n",
    "Recon0.columns=days_index\n",
    "Recon_k=[Recon0]\n",
    "print Recon0.shape\n",
    "\n",
    "for k in range(1,10):\n",
    "    Keig=Eig[:,:k]\n",
    "    Kprod=Prod[:,:k]\n",
    "    Recon=pd.DataFrame(Kprod*Keig.transpose() +KMean)\n",
    "    Recon.columns=days_index\n",
    "    Recon_k.append(Recon)\n",
    "    print k,Recon_k[k].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "print shape(Djoined.loc[i,days])\n",
    "print shape(Recon.loc[i,days])\n",
    "\n",
    "shape(Djoined),shape(Recon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "def plot_reconstructions(selection,rows=2,columns=7,size=3):\n",
    "    plt.figure(figsize=(columns*size,rows*size),dpi=300)\n",
    "    j=1;\n",
    "    for i in selection:\n",
    "        subplot(rows,columns,j); \n",
    "        j += 1; \n",
    "        if j>=rows*columns: \n",
    "            break\n",
    "        (Djoined.loc[i,days]).plot()\n",
    "        for k in range(3):\n",
    "            (Recon_k[k].loc[i,days]).plot()\n",
    "        title(Djoined.loc[i,'station']+' / '+str(Djoined.loc[i,'year']))\n",
    "plot_reconstructions(range(2000,2006),rows=10,columns=3,size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Observe in the reconstructions below that the blue line fills in (extrapolation/interpolation) the places where the measurements are not available. It also reduces the fluctuations in the relative to the original line. Recall the we are using the k top eigenvectors which explain about 88% of the variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:red\"> Check how the approximations change/improve as you increase the number of coefficients</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plot_reconstructions([2012],rows=2,columns=2,size=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "hist(Djoined.loc[:,'V0'],bins=100);\n",
    "title('Histogram of V0');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true,
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "selection= [i for i in range(shape(Djoined)[0]) if Djoined.loc[i,'latitude']<-10]\n",
    "plot_reconstructions(selection,rows=7,columns=3, size=5)\n",
    "shape(selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "<span style=\"color:red\">Can you reduce the reconstruction error (using a fixed number of eigenvectors) by splitting the stations according to region (for example country, state, latitudal range). Note that having a regions with very few readings defeats the purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "shape(np.array([Mean]).transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "321px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_position": {
    "height": "578px",
    "left": "0px",
    "right": "998.551px",
    "top": "105.994px",
    "width": "311px"
   },
   "toc_section_display": "block",
   "toc_window_display": true,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
